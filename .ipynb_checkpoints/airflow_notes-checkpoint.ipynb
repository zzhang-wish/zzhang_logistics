{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to test the DAG script in testing env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get into your folder where the data is stored in the terminal\n",
    "2. Then use this command to move your file into the local machine - scp dag_epc_channel_breakdown.py sshaukat@10.42.16.129:~/ContextLogic/clroot/wish_airflow/dags/\n",
    "3. After that ssh into your local machine - ssh 10.42.16.129\n",
    "4. Once you login to your local machine, go to this folder - cd ContextLogic/clroot/wish_airflow/\n",
    "5. then fab deploy_update\n",
    "6. it will ask you to download and install fab but the instructions will be there\n",
    "7.you can then go to http://airflow-testing.i.wish.com/admin/ and your dag should be available there for testing\n",
    "8. usually it takes 5-10 mins to show up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update_default_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (utilities.py, line 125)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-73bc68390dea>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from cl.utils.airflow_base.utilities import AirflowUtilities\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/zzhang/ContextLogic/clroot/cl/utils/airflow_base/utilities.py\"\u001b[0;36m, line \u001b[0;32m125\u001b[0m\n\u001b[0;31m    print \"Could not find the dependency results.\" \"Bucket : {bucket} , file : {path}\".format(\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from cl.utils.airflow_base.utilities import AirflowUtilities\n",
    "usecase : AirflowUtilities.update_default_args()\n",
    "parameters: \n",
    "    @classmethod\n",
    "    def update_default_args(cls, **kwargs):\n",
    "        \"\"\"\n",
    "        :return: Dict of default args updated with given kwargs\n",
    "        \"\"\"\n",
    "        default = {\n",
    "            \"email\": c.AIRFLOW_EMAIL_GROUP,\n",
    "            \"email_on_failure\": False,\n",
    "            \"email_on_retry\": False,\n",
    "            \"on_failure_callback\": cls.email_on_failure,\n",
    "            \"on_retry_callback\": cls.email_on_failure,\n",
    "            \"on_success_callback\": cls.on_success_callback,\n",
    "            \"depends_on_past\": c.DEFAULT_DEPENDS_ON_PAST,\n",
    "            \"retries\": c.DEFAULT_RETRIES,\n",
    "            \"retry_delay\": c.DEFAULT_RETRY_DELAY,\n",
    "            \"priority_weight\": c.DEFAULT_PRIORITY_WEIGHT,\n",
    "        }\n",
    "        \n",
    "        default.update(kwargs)\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "\n",
    "default_args = AirflowUtilities.update_default_args(\n",
    "    owner=\"sshaukat\",\n",
    "    #this means when you want the DAG run, at least to be yesterday.\n",
    "    start_date=datetime(2019, 6, 4),\n",
    "    ## no need on end_date, you can disable it through the UI website\n",
    "    pool=POOL.TESTING if DEBUG_MODE else POOL.PROD_DATA,\n",
    "    queue=QUEUE.TESTING if DEBUG_MODE else QUEUE.PROD_DATA,\n",
    "    ## today's run depends on yesterday's run for example.\n",
    "    depends_on_past=False,\n",
    ")\n",
    "\n",
    "OR\n",
    "\n",
    "DEFAULT_ARGS = AirflowUtilities.update_default_args(\n",
    "    owner=\"bwirakesuma\",\n",
    "    start_date=datetime.strptime(\"2018-09-16 12:00:00 AM\", \"%Y-%m-%d %I:%M:%S %p\"),\n",
    "    queue=QUEUE.PROD_DATA,\n",
    "    pool=POOL.PROD_DATA,\n",
    "    email=\"airflow-exceptions@contextlogic.com\",\n",
    "    depends_on_past=False,\n",
    "    retries=2,\n",
    "    retry_delay=timedelta(minutes=5),\n",
    "    priority_weight=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example:\n",
    "dag = DAG(\n",
    "    dag_id=\"EPCSubsidyBreakdown\",\n",
    "    default_args=default_args,\n",
    "    schedule_interval=\"@daily\",\n",
    "    ## If you disable today's DAG instance, once you enable it, do you need to backfill.\n",
    "    catchup=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example:\n",
    "def build_query(self, **kwargs):\n",
    "    ## this one indicates the the historical data horizon from and to.\n",
    "    end_date = kwargs[\"run_date\"]\n",
    "    start_date = datetime(2019, 4, 1)\n",
    "    # or relative date\n",
    "    base_date = kwargs[\"run_date\"]\n",
    "    end_date = base_date - timedelta(days=60)\n",
    "    start_date = end_date - timedelta(days=150)\n",
    "    \n",
    "    ## build SQL/Hive query below:\n",
    "    td_query = \"\"\"\n",
    "    INSERT INTO TABLE sweeper.logistics_delivered_tableau\n",
    "            SELECT  TD_TIME_FORMAT(td_date_trunc('week',CAST(coalesce(delivered,user_confirmed_delivery) AS bigint),'America/Los_Angeles'),'yyyy-MM-dd') AS week,\n",
    "                    .........\n",
    "                    sum(case when wsp.m_transaction_id is not null then 1 else 0 end) as orders\n",
    "\n",
    "            from sweeper.merch_merchanttransaction_dump wsp\n",
    "\n",
    "            left join (select channel, tracking_id from sweeper.merch_wishpost_events) b\n",
    "                on wsp.tracking_id = b.tracking_id\n",
    "\n",
    "            where wsp.confirmed_shipped is not null and coalesce(delivered,user_confirmed_delivery) >= {start_date}\n",
    "                  td_time_range(order_time,'{start_date_str}','{end_date_str}')\n",
    "            \n",
    "            group by 1,2,3,4,5\n",
    "    \"\"\".format(\n",
    "        start_date=int(start_date.timestamp()), \n",
    "        or start_date=start_date.date()\n",
    "        end_date_str=end_date.strftime(\"%Y-%m-%d\")\n",
    "        db=self.td_db, \n",
    "        target=self.target)\n",
    "\n",
    "    return td_query\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## another example with embedded queries\n",
    "\n",
    "def build_query(self, **kwargs):\n",
    "    current_date = kwargs[\"run_date\"]\n",
    "    end_date = current_date - timedelta(days=2)\n",
    "    contest_table = get_latest_sweeper_table(\"contest\")\n",
    "\n",
    "    impression_query = \"\"\"\n",
    "        SELECT first.cid,\n",
    "               first.name,\n",
    "               first.merchant_name,\n",
    "               first.mid,\n",
    "               first.bd_rep,\n",
    "               first.country_code,\n",
    "               first.categories,\n",
    "               first.subcategories,\n",
    "               first.imps_90D as imps_90D,\n",
    "               first.imps_30D as imps_30D,\n",
    "               first.imps_30D_60D as imps_30D_60D,\n",
    "               first.imps_7D as imps_7D,\n",
    "               first.imps_7D_14D as imps_7D_14D,\n",
    "               RANK() OVER(partition BY country_code ORDER BY imps_7D DESC) AS imp_rank_7D\n",
    "\n",
    "         FROM (SELECT  cid,\n",
    "                    c.name,\n",
    "                    cm.display_name as merchant_name,\n",
    "                    cm.id as mid,\n",
    "                    cm.bd_rep,\n",
    "                    country_code,\n",
    "                    categories,\n",
    "                    subcategories,\n",
    "                    SUM(case when x1.start_time >= {end_time} - 90*86400 and end_time < {end_time}\n",
    "                             then count\n",
    "                             else 0 end) as imps_90D,\n",
    "                    SUM(case when x1.start_time >= {end_time} - 30*86400 and end_time < {end_time}\n",
    "                             then count\n",
    "                             else 0 end) as imps_30D,\n",
    "                    SUM(case when x1.start_time >= {end_time} - 60*86400 and end_time < {end_time} - 30*86400\n",
    "                             then count\n",
    "                             else 0 end) as imps_30D_60D,\n",
    "                    SUM(case when x1.start_time >= {end_time} - 7*86400 and end_time < {end_time}\n",
    "                             then count\n",
    "                             else 0 end) as imps_7D,\n",
    "                    SUM(case when x1.start_time >= {end_time} - 14*86400 and end_time < {end_time} - 7*86400\n",
    "                             then count\n",
    "                             else 0 end) as imps_7D_14D\n",
    "\n",
    "            FROM sweeper.new_daily_country_segmented_impressions x1\n",
    "            LEFT OUTER JOIN sweeper.{latest_contest_table} c\n",
    "                ON x1.cid = c.`_id`\n",
    "            LEFT OUTER JOIN sweeper.commerce_merchant cm\n",
    "                ON c.merchant_id = cm.id\n",
    "            LEFT OUTER JOIN sweeper.merch_product_tags mpt\n",
    "                ON x1.cid = mpt.product_id\n",
    "            WHERE start_time >= {end_time} - 90*86400\n",
    "                AND x1.time >= {end_time} - 90*86400\n",
    "            GROUP BY 1,2,3,4,5,6,7,8) first\n",
    "    \"\"\".format(\n",
    "        latest_contest_table=contest_table, end_time=int(end_date.timestamp())\n",
    "    )\n",
    "\n",
    "    m_txn_query = \"\"\"\n",
    "        SELECT  product_id,\n",
    "                dest_country,\n",
    "                SUM(case when rating < 3 then 1 else 0 end) as low_rating,\n",
    "                SUM(case when rating is not null then 1 else 0 end) as rating_count,\n",
    "                AVG(rating) as avg_rating,\n",
    "                SUM(case when order_time >= {end_time} - 90*86400 and order_time < {end_time}\n",
    "                    then gmv\n",
    "                    else 0 end) as gmv_90D,\n",
    "                SUM(case when order_time >= {end_time} - 30*86400 and order_time < {end_time}\n",
    "                    then gmv\n",
    "                    else 0 end) as gmv_30D,\n",
    "                SUM(case when order_time >= {end_time} - 60*86400 and order_time < {end_time} - 30*86400\n",
    "                    then gmv\n",
    "                    else 0 end) as gmv_30D_60D,\n",
    "                SUM(case when order_time >= {end_time} - 7*86400 and order_time < {end_time}\n",
    "                    then gmv\n",
    "                    else 0 end) as gmv_7D,\n",
    "                SUM(case when order_time >= {end_time} - 14*86400 and order_time < {end_time} - 7*86400\n",
    "                    then gmv\n",
    "                    else 0 end) as gmv_7D_14D,\n",
    "                SUM(case when order_time >= {end_time} - 90*86400 and order_time < {end_time}\n",
    "                    then 1\n",
    "                    else 0 end) as orders_90D,\n",
    "                SUM(case when order_time >= {end_time} - 30*86400 and order_time < {end_time}\n",
    "                    then 1\n",
    "                    else 0 end) as orders_30D,\n",
    "                SUM(case when order_time >= {end_time} - 60*86400 and order_time < {end_time} - 30*86400\n",
    "                    then 1\n",
    "                    else 0 end) as orders_30D_60D,\n",
    "                SUM(case when order_time >= {end_time} - 7*86400 and order_time < {end_time}\n",
    "                    then 1\n",
    "                    else 0 end) as orders_7D,\n",
    "                SUM(case when order_time >= {end_time} - 14*86400 and order_time < {end_time} - 7*86400\n",
    "                    then 1\n",
    "                    else 0 end) as orders_7D_14D\n",
    "        FROM sweeper.merch_merchanttransaction mt\n",
    "            WHERE order_time >= {end_time} - 90*86400\n",
    "        GROUP BY 1,2\n",
    "     \"\"\".format(\n",
    "        end_time=int(end_date.timestamp())\n",
    "    )\n",
    "\n",
    "    query = \"\"\"\n",
    "             INSERT INTO TABLE {db}.{target}\n",
    "                    SELECT  y.*,\n",
    "                            z.low_rating,\n",
    "                            z.rating_count,\n",
    "                            z.avg_rating,\n",
    "                            z.gmv_90D,\n",
    "                            z.gmv_30D,\n",
    "                            z.gmv_30D_60D,\n",
    "                            z.gmv_7D,\n",
    "                            z.gmv_7D_14D,\n",
    "                            z.orders_90D,\n",
    "                            z.orders_30D,\n",
    "                            z.orders_30D_60D,\n",
    "                            z.orders_7D,\n",
    "                            z.orders_7D_14D\n",
    "                    FROM ({impression_query}) y\n",
    "                    LEFT OUTER JOIN ({m_txn_query}) z\n",
    "                        ON y.cid = z.product_id\n",
    "                        AND y.country_code = z.dest_country\n",
    "                    WHERE imp_rank_7D <= 10000\n",
    "\n",
    "    \"\"\".format(\n",
    "        impression_query=impression_query,\n",
    "        m_txn_query=m_txn_query,\n",
    "        db=self.td_db,\n",
    "        target=self.target,\n",
    "    )\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDQueryOperator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cl.utils.airflow_base.operators import TDQueryOperator\n",
    "\"\"\"\n",
    "        Runs a TD Query which inserts data into a TD table.\n",
    "        This task does not Return any results to upstream\n",
    "\n",
    "        :param target: results td table name [required]\n",
    "        :param td_db: results td database name [default: sweeper]\n",
    "\n",
    "        :param build_query: build_query(self, **kwargs) [required]\n",
    "        :param pre_process: pre_process(self, **kwargs)\n",
    "        :param post_process: post_process(self, **kwargs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example:\n",
    "def prep_fact_table(self, **kwargs):\n",
    "    table_delete(self.td_db, self.target)\n",
    "    table_create(self.td_db, self.target)\n",
    "\n",
    "task1 = TDQueryOperator(\n",
    "        task_id=\"EPCSubsidyBreakdown\",\n",
    "        dag=dag,\n",
    "        build_query=build_query,\n",
    "        pre_process=prep_fact_table,\n",
    "        marker=MARKER, or marker={\"TDMarker\": TDMarker},\n",
    "        ## table name\n",
    "        target=\"daily_epc_subsidy_breakdown\",\n",
    "        td_db=\"sweeper\",\n",
    "        ## True = OVERWRITE, False = Append\n",
    "        truncate=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDToRedshiftLoadUnionOperator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cl.utils.airflow_base.operators import TDToRedshiftLoadUnionOperator\n",
    "\"\"\"\n",
    "        :param distall: boolean if distall [default: False]\n",
    "        :param time_key:\n",
    "        :param suffix: redshift\n",
    "        :param lookback: redshift lookback\n",
    "        :param lookforward: redshift lookforward\n",
    "        :param source_db: td database [default: sweeper]\n",
    "        :param schema_name: redshift schema [default: public]\n",
    "        :param sortkey_field: redshift sort key\n",
    "        :param partition_key: redshift partition key\n",
    "        :param cluster: redshift cluster [default: wish-platform]\n",
    "        :param table_name: redshift table name [required]\n",
    "        :param fields_list: list of redshift fields [required]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example:\n",
    "    union_load_task = TDToRedshiftLoadUnionOperator(\n",
    "                        task_id=\"TDRedshiftUnion\",\n",
    "                        dag=dag,\n",
    "                        marker=MARKER,\n",
    "                        source_db=\"sweeper\",\n",
    "                        #the schema value in Tableau, \"logistics\" for example\n",
    "                        schema_name=\"logistics\",\n",
    "                        table_name=\"daily_epc_subsidy_breakdown\",\n",
    "                        #predefined the redshift schema\n",
    "                        fields_list=FIELDS_LIST,\n",
    "                        ## True = OVERWRITE, False = Append\n",
    "                        truncate=True,\n",
    "                        dependencies=[task1],\n",
    "                        ## use below one as default value\n",
    "                        cluster=c.RS_WISH_PLATFORM,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TableauWorkbookRefreshOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cl.utils.airflow_base.operators import TableauWorkbookRefreshOperator\n",
    "\"\"\"\n",
    "   Refreshes a tableau data extract\n",
    "   :param workbook_name: full name of tableau workbook to refresh\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example:\n",
    "    tableau_load_task = TableauWorkbookRefreshOperator(\n",
    "                        task_id=\"EPCDashboard_TableauExtract\",\n",
    "                        dag=dag,\n",
    "                        ## this is requried, and you can upload more than 1 tables to same Tableau workbook \n",
    "                        workbook_name=\"EPC Dashboard\",\n",
    "                        marker=MARKER,\n",
    "                        dependencies=[union_load_task],\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
